# 用例 #403：AI 机器学习模型文档生成器

**角色**：数据科学家 / 机器学习工程师 | **行业**：SaaS、科技、企业 | **任务**：文档、模型治理、知识管理

---

## 详细介绍

**痛点：永远写不完的文档**

ML 模型文档在数据科学工作流中占据着人人都认为重要、几乎没人优先处理的位置。这种模式在各组织中保持一致：构建模型的数据科学家对其了如指掌——训练数据特征、特征工程决策、超参数选择、导致最终配置的性能权衡、模型失败的已知边缘案例、预测有效所必须满足的部署假设。这些知识完全存在于一个人的脑海中。当那个人离开、转到另一个团队或被重新分配时，模型就成了一个在生产环境中运行的黑盒，没有人理解如何评估其行为、如何正确重训练，或者如何识别它何时已经降级。

确实存在的文档几乎总是不完整的。模型仓库中的 README 文件可能记录了训练命令和最终评估指标。JIRA 票据可能包含原始需求。Confluence 页面可能有为非技术利益相关方编写的高层次描述。系统性缺失的内容包括：训练数据架构及其覆盖的时间段、特征定义及其计算逻辑、训练前应用的数据质量检查、按受保护特征的子群体性能分解、已知模型失败模式及触发它们的输入条件、应触发重训练的监控阈值，以及服务基础设施必须维护的部署假设。这些都是接管模型维护的任何工程师或数据科学家需要回答的常规运营问题——没有文档，他们就通过阅读代码来回答，这很慢、容易出错，而且当训练管道自模型最初构建以来已经改变时根本不可能。

文档赤字在到达审查流程时会造成具体、可量化的危害。金融机构根据 SR 11-7 进行的监管审计、企业风险职能要求的模型审查、内部 AI 伦理政策授权的公平性审查，以及企业客户进行的供应商评估，都需要大多数数据科学团队无法从现有记录中生成的模型文档。响应是被动的：文档在安排审计时匆忙从代码、会议记录和 Slack 对话中拼凑起来，而非作为模型生命周期的活性工件进行维护。这种被动文档通常不完整、格式不一致，并且不反映模型的当前状态——它反映的是文档作者在时间压力下的最佳记忆。

根本原因不是缺乏对文档重要性的认识。数据科学家知道它重要。根本原因是在交付压力下撰写好的模型文档需要将技术决策转化为结构化的散文，而没有任何工具、模板或工作流能使其快速到足以在进入下一个项目之前完成。一个生产 ML 模型的全面模型卡片需要回答 40-60 个关于训练数据、特征工程、模型架构、评估方法论、公平性分析、部署要求和监控规范的结构化问题。在任何时间压力下从头写这些都是一项重大工程，始终输给下一轮模型训练。

**COCO 如何解决这一问题**

COCO 作为专业文档伙伴加速模型文档化——用正确的问题提示数据科学家，将答案汇编成结构化文档工件，并将技术实施细节转化为同时服务于技术和非技术读者的散文。

1. **模型卡片生成**：COCO 遵循 Mitchell 等人（2019 年）模型卡片框架和 Hugging Face 模型卡片标准生成全面的模型卡片——涵盖模型细节、预期用途、影响性能的因素、评估结果和伦理考虑。
   - 提示数据科学家以结构化形式提供训练数据、特征和评估信息，然后汇编叙事文档
   - 同时生成完整的技术模型卡片和适合利益相关方审查的高管级摘要

2. **训练数据文档（数据表）**：COCO 遵循 Gebru 等人（2018 年）数据集数据表框架生成训练数据文档——涵盖数据收集、构成、预处理、用途、分发和维护。
   - 记录训练数据架构、时间覆盖范围、已知偏见和局限性，以及训练集可能不具代表性的条件
   - 规定训练前应用的数据质量过滤器以及每个过滤器删除的记录比例

3. **特征定义目录**：COCO 生成特征文档，定义每个输入特征——其业务含义、计算逻辑、数据来源、刷新节奏、预期值范围和已知数据质量问题——格式同时支持模型审计和特征复用。
   - 识别可能编码受保护特征的特征（代理歧视风险）
   - 记录特征重要性排名和顶级特征的业务解释

4. **子群体性能分析文档**：COCO 构建按相关子群体分类的模型性能文档——确保跨人口统计或行为细分的性能差异被记录，而不仅仅是聚合指标。
   - 以标准化格式模板化子群体分析结果，涵盖每个子群体的精度、召回率和假阳性/阴性率
   - 生成性能差异及其运营含义的通俗语言解释

5. **部署和基础设施要求**：COCO 记录服务要求，这些要求必须维护以使模型产生有效预测——涵盖特征服务延迟要求、基础设施依赖关系、模型版本控制和回滚程序。
   - 规定模型依赖的生产信号以及触发回滚的降级情况
   - 记录在完全生产切换前所需的 A/B 测试和影子部署程序

6. **监控规范和告警设计**：COCO 生成监控文档，规定要跟踪哪些指标、哪些告警阈值表示降级，以及每种告警类型采取什么补救措施。
   - 定义数据漂移检测方法论、模型性能监控节奏和重训练触发条件
   - 记录模型相关生产告警的值班运行手册

**可量化的成果**

- **模型部署时的文档完整性**：在部署时具有完整文档（模型卡片、特征目录、监控规范）的生产模型比例 → 基线 8% → COCO 辅助文档工作流采用后 64%
- **完成模型卡片的时间**：每个模型生成完整模型卡片所需的小时数 → 平均 12 小时 → COCO 结构化提示和草稿生成后 2.8 小时
- **审计准备度**：为监管审查汇编文档包所需的时间 → 3-4 周被动汇编 → COCO 文档化模型按需可用
- **模型交接事件**：归因于模型交接期间知识差距的生产事件 → 每季度 3.2 个事件 → 文档标准采用后每季度 0.7 个
- **特征复用率**：为一个模型构建的特征后来在另一个模型中复用的比例 → 11% → 特征目录文档实现发现后 34%

**受益角色**

- **数据科学家**：在数小时而非数天内完成模型文档，并创建工件，保护其工作在交接时不被误用或误解
- **ML 工程师**：使用具体且可验证的部署和监控规范做出服务基础设施决策，而非从训练代码中推断
- **AI 伦理和公平性审查员**：以标准化格式访问子群体性能分析文档和特征代理分析，实现系统性公平性审查
- **风险和合规团队**：收到满足 SR 11-7 模型风险管理要求、欧盟 AI 法案文档义务或企业 AI 治理政策要求的模型文档包，而无需数据科学家了解监管术语

---

## 实用提示词

**提示词 1：完整模型卡片生成**
```
帮我为我构建的机器学习模型生成一个全面的模型卡片。

模型基础信息：
- 模型名称和版本：[名称 v.X.X]
- 模型类型：[算法 / 架构——例如"XGBoost 二分类器"、"微调的 BERT"]
- 任务：[模型做什么——例如"预测 SaaS 客户的 30 天流失概率"]
- 主要利益相关方：[谁使用模型输出]
- 部署背景：[模型在哪里以及如何部署]

训练数据：
- 来源：[数据来源]
- 覆盖时间段：[日期范围]
- 训练样本数量：[N]
- 标签定义：[目标如何定义]
- 训练数据的已知局限性或偏见：[描述]

模型性能（提供您的评估结果）：
- 整体指标：[精度、召回率、F1、AUC 等]
- 子群体性能：[按细分的性能（如可用）]
- 基线比较：[模型超越的基准]

生成一个完整的模型卡片，涵盖：
1. 模型细节（架构、训练方法、超参数）
2. 预期用途和范围外用途
3. 训练数据摘要和局限性
4. 带子群体分解的评估结果
5. 伦理考虑和已知风险
6. 适当使用的注意事项和建议
7. 高管摘要（非技术性，最多 200 字）
```

**提示词 2：训练数据数据表**
```
为用于训练我的 ML 模型的数据集生成一个训练数据文档数据表。

数据集基础信息：
- 数据集名称：[名称]
- 代表什么：[每行是什么——例如"每位客户每月一行，代表客户在每月初的状态"]
- 大小：[N 行，N 列，覆盖日期范围]
- 来源系统：[数据来自哪里]
- 如何构建：[应用的连接、聚合、过滤]

数据特征：
- 标签来源：[标签如何生成]
- 已知类别不平衡：[正负样本比例]
- 发现的数据质量问题：[空值、重复、不一致及处理方式]
- 训练前应用的过滤器：[排除了哪些记录以及原因]
- 潜在偏见：[过度或不足代表的人群或时间段]

生成一个数据表，涵盖：
1. 动机（为什么创建这个数据集，谁资助/创建了它）
2. 构成（包含什么，如何收集）
3. 收集过程（抽样方法，时间段）
4. 预处理和清洗（应用了什么转换）
5. 用途（适用于什么，不应该用于什么）
6. 分发（如何访问，访问控制）
7. 维护（如何保持最新，谁负责）
```

**提示词 3：特征定义目录**
```
为我的 ML 模型中使用的特征生成一个特征定义目录。

模型：[模型名称]
特征列表（提供您拥有的尽可能多的细节）：
对于每个特征，提供：名称、描述、数据来源、计算方法、预期范围和任何已知问题。

特征 1：
- 名称：[特征名称]
- 业务含义：[代表什么]
- 计算方法：[如何计算——SQL/公式]
- 来源表/字段：[来源]
- 预期值范围：[最小值，最大值，分布]
- 训练数据中的空值率：[%]
- 已知数据质量问题：[任何问题]

[对每个特征重复]

对每个特征，记录：
1. 标准化定义（业务描述 + 技术规范）
2. 特征类型（数值、分类、二进制、嵌入等）
3. 代理歧视潜力（此特征是否与受保护特征相关？）
4. 特征重要性排名和解释
5. 已知失败模式（此特征变得不可靠或无效的条件）
6. 生产中的刷新节奏和服务延迟要求
7. 依赖关系（此特征依赖的其他特征或上游数据资产）
```

**提示词 4：监控规范和运行手册**
```
为我的生产 ML 模型生成模型监控规范和运营运行手册。

模型：[模型名称和版本]
部署：[模型在哪里运行——批量评分 / 实时 API / 等]
预测目标：[模型输出什么以及如何使用]
模型失败的业务影响：[如果模型停止工作或降级，什么会出错]

当前监控设置（如有）：[您已经在监控什么]
重训练节奏：[您当前多久重训练一次]
数据刷新节奏：[输入特征多久更新一次]

生成一个监控规范，涵盖：
1. 要跟踪的性能指标：哪些指标、测量频率和每个指标的数据要求
2. 数据漂移监控：哪些输入特征要监控分布偏移，检测方法（PSI、KS 检验等）和告警阈值
3. 标签漂移监控：当真实标签延迟时如何监控预测分布偏移
4. 告警阈值：对每个监控指标，定义警告阈值（调查）、关键阈值（升级）和紧急阈值（回滚）
5. 值班运行手册：对每种告警类型——调查步骤、补救选项（重训练、回滚、特征刷新）和升级路径
6. 重训练触发条件：需要模型重训练的明确标准（不仅仅是基于日历的）
7. 模型更新的影子部署和 A/B 测试程序
```

**提示词 5：子群体性能分析文档**
```
帮我为我的 ML 模型的公平性和子群体性能分析生成文档，格式适合内部 AI 伦理审查和外部监管审计。

模型：[模型名称]
模型用例：[输出如何使用以及它们影响哪些决策]
数据中的潜在敏感属性：[列表——例如"年龄、性别、地区、账户类型"]
模型影响的业务决策：[根据模型分数采取什么行动]

我拥有的性能结果（提供您测量的内容）：
- 整体：[精度 / 召回率 / F1 / AUC]
- 子群体 1 [描述群体]：[指标]
- 子群体 2 [描述群体]：[指标]
- [对每个分析的子群体继续]

生成一个子群体性能文档包，涵盖：
1. 分析方法论：分析了哪些群体、为什么选择这些群体，以及使用了什么指标
2. 结果表：显示每个子群体在所有指标上性能的标准化格式
3. 不利影响分析：对每个指标，标记性能与整体差异超过 [X%] 阈值的子群体
4. 根本原因分析：对每个显著差异，可能的原因是什么（训练数据代表性、特征代理效应、标签偏见）？
5. 风险评估：如果模型部署，每个已识别差异的业务和伦理风险是什么？
6. 缓解选项：哪些方法（重新加权、阈值调整、额外数据收集）可以减少每个差异？
7. 监控建议：生产监控中应跟踪哪些子群体指标？
```

---
