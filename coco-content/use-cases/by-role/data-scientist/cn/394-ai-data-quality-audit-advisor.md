# #394 — AI 数据质量审计顾问

**角色**：数据科学家
**行业**：SaaS / 科技
**任务**：分析
**标识**：`ai-data-quality-audit-advisor`

---

## 简介

数据质量问题是机器学习部署失败的首要原因，然而大多数团队在模型已经上线后才发现这些问题。2024 年对 200 家公司 ML 工程师的调查显示，73% 的团队曾遭遇由数据质量下降引发的生产事故——其中 61% 表示，问题在被发现前已在训练数据中存在数周甚至数月。这种延迟发现的代价极为惨重：从数据质量事故到生产环境中检测到模型退化，平均间隔 47 天，期间模型以更差的结果悄无声息地大规模运行。

挑战在于，ML 语境下的数据质量从根本上比传统数据仓库更为复杂。传统数据质量检查验证值是否在预期范围内、外键是否有效、空值率是否可接受。ML 数据质量还需额外考量：标签质量（目标变量是否正确？）、特征-目标泄漏（特征是否包含未来信息？）、训练与推理间的分布漂移（用户群体是否发生变化？）、表示性偏差（重要子群体是否系统性地被低估？）、时效性（特征是否在预测时点被正确计算？）以及模式漂移（上游系统是否在未通知的情况下更改了列的格式或语义？）。

高速增长 SaaS 公司的数据科学家通常会接手并非自己构建的数据集，这些数据集有未记录的转换、含义模糊的列名，以及关于数据最初如何清洗的无从考证的历史。新加入团队的数据科学家可能收到一个 200 列的训练数据集，配一条 Slack 消息说"这是我们上一个模型用的，你应该可以直接用"。审计这个数据集以了解其质量、局限性和对新建模任务的适用性，可能需要两到三周的仔细调查——而实际上往往被压缩到两到三天，结果是在一堆未经审查的假设基础上构建模型。

COCO 充当系统化的数据质量审计向导，帮助数据科学家更快地完成彻底的审计，并以能随数据集流传的格式记录发现。工作流程如下：

1. **描述数据集及其预期用途。** 分享模式、数据来源、采集方法、目标变量以及你打算解决的预测任务。
2. **分享初步分析的样本输出。** 粘贴来自 pandas-profiling、Great Expectations 或简单 `.describe()` 和 `.value_counts()` 的结果。
3. **COCO 生成结构化审计清单。** 根据你的数据集类型和 ML 任务定制，涵盖：完整性、一致性、及时性、泄漏风险、标签质量、分布属性和偏差指标。
4. **执行审计并分享发现。** 在逐项检查清单时，分享你的发现。COCO 帮助解读模糊的发现（"23% 的空值率——是有意义的问题还是预期现象？"）并对需要调查的问题发出警告。
5. **生成数据质量报告。** COCO 产出结构化报告，记录发现、严重等级评定、建议的缓解措施和适用性评估。

使用 COCO 进行数据质量审计的团队报告，平均每个数据集发现 3.7 个原本会进入模型训练的关键问题。在受监管行业，COCO 生成的结构化审计文档直接满足了模型治理的合规要求。

**受益角色：**

- **数据科学家**：接手其他团队的数据集，需要在建模前了解其质量
- **ML 工程师**：构建自动化数据验证流水线，需要有关检查项目的参考
- **分析工程师**：负责流经 dbt 流水线进入 ML 特征存储的数据质量
- **首席数据官和数据治理团队**：需要系统化的质量文档用于合规和审计目的

---

## 实用提示词

**提示词 1 — 全面数据质量审计清单**
```
我需要审计一个数据集的 ML 建模适用性。背景如下：

数据集描述：[数据代表什么]
数据来源：[来自哪里——如"BigQuery 中的 CRM 事件表"、"S3 中的 API 日志"]
预期 ML 任务：[要构建什么——如"二分类流失预测器"]
预测粒度：[如"每客户每月一次预测"]
目标变量：[目标，如何定义]

模式（最重要的列）：
- [列名]：[类型]，[描述]，空值率：[%]
- [列名]：[类型]，[描述]，空值率：[%]
[继续]

初步分析摘要：
[粘贴 pandas-profiling 摘要或 .describe() 输出]

请生成结构化数据质量审计清单，涵盖：完整性、有效性、一致性、及时性、唯一性、泄漏风险、标签质量和分布健康度。针对每个维度，给出需要运行的具体检查和需要注意的警示信号。
```

**提示词 2 — 空值与缺失数据分析**
```
我的 ML 训练数据集存在大量缺失数据，需要帮助判断是否以及如何处理它。

数据集：[描述]，n=[N 行]，[N 个特征]
ML 任务：[任务]

缺失数据概况：
| 特征 | 缺失% | 缺失模式 | 备注 |
|------|-------|----------|------|
| [F1] | [%]   | [随机/系统性/按片段] | [备注] |
| [F2] | [%]   | [模式]   | [备注] |
[继续列出缺失率 >5% 的特征]

缺失值之间的相关性：[缺失值是否共现？如已知请描述]

请分析：(1) 每个特征的缺失是 MCAR/MAR/MNAR 以及为什么重要，(2) 每个特征的插补策略建议，(3) 是否有任何缺失数据模式揭示了需要从源头修复的数据采集 bug，(4) 如何创建缺失性指示特征，(5) 如何验证插补不会对模型引入偏差。
```

**提示词 3 — 分布漂移检测**
```
我想检查训练数据分布与生产环境中模型服务人群之间是否存在显著差异。

训练数据：
- 时间范围：[日期范围]
- 来源：[描述]
- n=[N 行]

生产推理人群（已知信息）：
- 时间范围：[日期范围]
- 来源：[描述]
- n=[N 行或"未知"]

我怀疑存在漂移的特征：
[列出已知或疑似存在分布差异的特征]

可用于比较的数据：
[粘贴两个数据集中关键特征的统计摘要、直方图或值分布]

对每个特征：(1) 量化分布漂移（根据适用性使用 KL 散度、PSI 或 Kolmogorov-Smirnov），(2) 评估漂移是否足以损害模型性能，(3) 建议缓解措施（重新加权、重新采集或架构变更），(4) 优先排序在部署前最需要解决的漂移问题。
```

**提示词 4 — 标签质量评估**
```
我担心训练标签的质量，需要评估标签噪声的程度以及应对措施。

目标变量：[目标描述]
标签生成方式：[流程——如"人工标注"、"代理事件（订阅取消）"、"基于 CRM 状态的规则"]
已知的标签生成问题：[任何你怀疑的问题——如"部分取消是自动续费失败，并非真正的流失"]

数据集：n=[N 行]，正例比率：[%]

标签质量问题的证据：
[描述任何异常——如"特征 X 高度预测但逻辑上不应如此"、"模型置信度很高但业务不相信这些预测"]

请评估：(1) 可能的标签噪声率及其对模型质量的影响，(2) 检测和清洗标签噪声的方法（置信学习、交叉验证分歧等），(3) 标签定义本身是否需要修改，(4) 如何量化不可减少的标签噪声带来的性能上限损失。
```

**提示词 5 — 数据质量报告生成**
```
我完成了对 [数据集名称] 数据集的质量审计，需要将发现记录在结构化报告中。

审计发现：

关键问题（训练前必须修复）：
1. [问题描述，严重程度，受影响的行/特征，根本原因]
2. [问题描述]

中等问题（应修复，不修复影响较小）：
1. [问题描述]
2. [问题描述]

轻微问题（记录并监控）：
1. [问题描述]

数据集优势：
- [正面发现1]
- [正面发现2]

适用性评估：[您的整体评估]

请生成正式的数据质量报告，适用于：(1) 需要修复源头问题的数据工程团队，(2) 需要决定是否继续推进的 ML 团队负责人，(3) 合规或审计存档。包含严重等级评定、建议的修复措施，以及关于是否开始模型训练的建议。
```
