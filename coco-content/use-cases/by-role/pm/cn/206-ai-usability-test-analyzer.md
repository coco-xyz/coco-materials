# 用例 #206：AI可用性测试分析器

**角色**：产品经理 / UX研究员 | **行业**：SaaS、电商、消费级应用、企业软件 | **任务**：可用性测试、用户体验摩擦点分析、会话数据处理

---
## 详细介绍

**痛点：测试做完了，真正的工作才刚刚开始**

跑可用性测试本身是最简单的环节。一个5人团队用两天时间完成8场有主持的测试，会产生大约12小时的屏幕录像、标注好的点击路径、任务完成日志，以及"出声思考"的语音记录。最后一场结束时，研究团队的下一个项目早已排队等候。而分析工作——回看录像、标注摩擦时刻、量化任务成功率、跨参与者梳理行为规律——还需要一周甚至更多时间。本该快速的反馈闭环，变成了一场耗时且昂贵的报告生产工程。

当规模扩展到无主持测试时，问题会进一步放大。通过UserTesting或Maze等平台收集50到200份无主持测试数据，所产生的信息量远超大多数团队的处理能力。团队只能从中挑选有时间看的部分进行回顾，引入了严重的选择偏差，大量行为信号被白白浪费。所谓"可用性发现"往往只反映团队处理能力所及的范围，而非数据本身所揭示的真实情况。

与此同时，产品和设计决策悬而未决。开发人员在等设计方案，设计师在等研究结论。分析拖得越久，产品就越有可能在发现结果落地之前已经推进，整个测试投入也就部分打了水漂。

**COCO如何解决**

COCO的AI可用性测试分析器能够处理多模态会话数据——点击路径、任务日志、完成时间戳、错误事件，以及语音和文字反馈——以极快的速度精准识别用户体验摩擦点。

1. **任务完成率分析**：自动计算测试协议中每个任务在所有会话中的成功、失败和部分完成率。
   - 区分关键失败（放弃任务）、险过（路径偏差但最终成功）和顺畅完成
   - 将低于可配置阈值（如70%以下）的任务完成率标记为高优先级摩擦区
   - 按参与者细分（新手与有经验用户、不同设备类型）展示完成率差异

2. **点击路径与导航偏差检测**：将用户实际操作路径与预期最优路径进行比对，识别用户"偏轨"的位置。
   - 绘制用户偏离预期流程的最常见路径
   - 识别"逃生出口"——用户迷失或困惑时点击的元素
   - 量化实际步骤数与最少所需步骤数之比（路径效率评分）

3. **摩擦时刻聚类**：将犹豫停顿、错误事件和回退行为按页面、步骤和用户类型进行聚类分组。
   - 识别触发最多困惑的具体UI元素或交互模式
   - 将摩擦时刻与出声思考会话中的语音评论进行关联
   - 按频率（发生频次）和影响程度（对任务完成的干扰程度）对摩擦严重性排序

4. **出声思考情感与困惑度映射**：分析会话中的语音或文字反馈，提取情感倾向和困惑信号。
   - 识别用户表达挫败感、惊讶或不确定的时刻
   - 将语音评论与特定页面状态或交互事件关联
   - 生成"困惑地图"，将用户语言与UX摩擦点叠加呈现

5. **跨会话规律综合分析**：综合所有会话的发现，将系统性UX问题与个体异常值区分开来。
   - 区分影响大多数用户的问题与仅影响特定用户类型的问题
   - 挖掘只有在20+场会话中才能浮现的行为规律
   - 产出按严重程度排序的问题清单：关键（影响多数用户、阻断任务）、中等、轻微

6. **设计改进建议生成**：将摩擦点发现转化为可直接交付设计师的可操作改进假设。
   - 针对每个已识别的摩擦点，基于观察到的行为提出1到2个设计改进方向
   - 将发现格式化为可验证的假设："我们认为[设计改变]将会[降低摩擦指标]，因为[会话中的证据]"

**量化结果**

- **分析时间**：从手动5到7天 → COCO辅助6到8小时（效率提升80%以上）
- **会话覆盖率**：从回顾约30%的会话 → 分析100%的数据
- **发现的摩擦点数量**：每轮测试周期中发现的独立UX问题增加2.5倍
- **设计交接周期**：从10天缩短至2天以内
- **误判（错误归因的用户操作失误）**：通过跨会话规律验证，减少约40%
- **因等待研究结果导致的迭代延期**：在迭代周期内使用COCO的团队基本消除此问题

**谁会受益**

- **产品经理**：无需等待一周的分析周期，直接获得有证据支撑、按优先级排序的UX摩擦报告，可立即转化为迭代任务
- **UX设计师**：收到具体的、有会话记录支撑的摩擦点和行为证据——不只是"用户感到困惑"，而是精确到哪里困惑、为何困惑
- **UX研究员**：将精力聚焦于会话主持和测试方案设计，而非耗费大量时间手动回顾每一场会话
- **开发团队**：获得基于行为数据的明确设计需求，减少UX修复过程中的来回沟通

---

## 实用提示词

**提示词1：全量可用性测试会话分析**
```
我刚刚完成了针对 [公司名称] [产品/功能名称] 的一轮可用性测试。
以下是会话数据，请分析并输出结构化的可用性发现报告。

测试方案：
- 会话数量：[例如：12场有主持 / 80场无主持]
- 测试任务：[列出参与者需要完成的3到5个任务]
- 用户细分：[描述参与者画像]
- 测试平台/方式：[例如：Maze、UserTesting、线下有主持]

数据说明：[描述所附内容——点击路径导出、会话录像、完成日志、出声思考转录文本]

请输出：
1. 逐任务完成率拆解（成功/部分完成/失败），以及每个任务的主要摩擦时刻
2. 前5到8个UX摩擦点，按严重程度和频率排序，附支撑证据
3. 优先级最高的3个问题——修复后对整体任务成功率提升最大
4. 能够区分系统性导航或标签问题与孤立UI问题的行为规律
5. 针对每个主要摩擦点的设计假设，格式为："我们认为[改变]将会[结果]，因为[证据]"
```

**提示词2：点击路径偏差分析**
```
请分析我们近期针对 [功能/流程名称] 进行的可用性测试的点击路径数据。

预期最优流程：
第1步：[页面/操作]
第2步：[页面/操作]
第3步：[页面/操作]
第4步：[页面/操作]
[根据实际情况继续]

实际点击路径数据：[粘贴导出内容或说明附件情况]

我想了解：
1. 完全按照最优路径操作的用户比例是多少？
2. 用户最常在哪里偏离，偏离后点击了什么？
3. 实际路径长度与最优路径长度之比（效率评分）是多少？
4. 哪些"错误路径"最终仍能完成任务，哪些导致了放弃？
5. 是否存在某些非预期路径实际效果很好的情况——这或许说明我们的最优流程并非真正最优？

会话背景：[会话数量、用户类型、设备类型（如相关）]
```

**提示词3：出声思考转录文本摩擦度映射**
```
我有来自 [产品领域] [场数] 场可用性测试的出声思考转录文本。
请分析语音反馈，构建摩擦点与困惑度地图。

针对每份会话转录文本：
1. 识别用户表达困惑、挫败感或不确定性的所有时刻——记录他们当时描述的具体页面或操作
2. 提取表明心智模型偏差的语言（例如："我以为这个按钮会……"、"这个在哪里……"）
3. 情感标注：正面、中性、困惑、挫败

然后跨所有会话进行综合分析：
4. 哪些页面或交互点引发了最多负面/困惑的语音反应？
5. 用户描述产品所用的语言与我们的标签有哪些差异？（标签/分类体系偏差）
6. 是否存在超出预期的正面反应时刻——值得我们进一步强化？

[请在下方粘贴转录文本或说明附件情况]
测试的产品领域：[简要描述]
```

---
