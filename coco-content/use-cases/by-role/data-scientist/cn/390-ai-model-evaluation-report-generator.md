# #390 — AI 模型评估报告生成器

**角色**：数据科学家
**行业**：SaaS / 科技
**任务**：报告生成
**标识**：`ai-model-evaluation-report-generator`

---

## 简介

模型评估是机器学习生命周期中最关键的环节之一，却也是文档化最为薄弱的一环。一位在中型 SaaS 公司工作的数据科学家，可能花费两到三周时间训练梯度提升分类器、使用 Optuna 调优超参数、并跨五折交叉验证——最后却只用四十五分钟写一页 PPT，把所有严谨的研究工作压缩成一个准确率数字。利益相关方就凭这个数字来批准或否定模型，完全不了解置信区间、决策阈值敏感性、类别不平衡的影响，或假负例与假正例各自的业务成本。

这不是懒惰，而是现实困境。一份完整的模型评估报告需要涵盖：整体性能指标（准确率、精确率、召回率、F1、AUC-ROC、对数损失）、分类别明细、混淆矩阵解读、校准分析、特征重要性排名、与基线或上一版本的对比、训练集与测试集之间的数据分布漂移分析、不同决策阈值下的敏感性分析，以及每项指标的业务影响解读。从零开始撰写这份报告，即使是经验丰富的从业者也需要三到五小时——而且不同项目之间的报告格式往往不一致，导致跨团队横向比较几乎无从实现。

由此产生的下游后果是真实存在的：模型在没有明确性能合同的情况下上线，遇到分布外数据时出现事故，管理层根据误导性摘要做出预算决策。一项针对企业软件公司 ML 团队的研究发现，68% 的团队表示其模型文档不足以支撑部署后的问题排查，54% 的团队承认他们在上线某个模型时，自己对该模型的理解并不完整。

COCO 通过充当结构化的评估协作撰写者来解决这一问题。当你将指标、混淆矩阵和实验配置粘贴给 COCO 时，它不仅仅是重新排版你的数字——它会解读数字、结合行业基准提供背景、标记潜在风险，并根据受众（技术评审、高管摘要或合规审计）生成完整的叙述性报告。整个流程分四步进行：

1. **导入原始输出。** 将 scikit-learn 的 `classification_report`、MLflow 运行元数据、验证损失曲线和阈值扫描结果直接粘贴到 COCO 提示中。
2. **指定受众和背景。** 告知 COCO 报告是用于内部模型评审会议、高管产品决策、合规审计还是对外发布——每种受众需要不同的框架。
3. **COCO 起草完整结构化报告。** 生成内容包括：执行摘要、方法论概述、逐指标分析与业务解读、风险标记、后续建议步骤，以及（如提供历史版本指标）对比表格。
4. **针对特定章节迭代。** 如果精确率-召回率权衡需要更多解释，或者需要将特征重要性章节改写为非技术受众版本，可以单独要求 COCO 修改该章节，同时保持其余内容不变。
5. **导出分享。** 最终报告为整洁的 Markdown 或结构化文本，可直接导入 Notion、Confluence 或 Google 文档，无需二次排版。

使用此工作流的团队报告，评估报告撰写时间从平均 4.2 小时缩短至 35 分钟，减少 86%。更重要的是，报告内容更完整：COCO 会一致地包含校准分析和阈值敏感性章节——这些内容在手工撰写时工程师通常会略去，因为用文字解释起来繁琐，但对生产使用至关重要。

**受益角色：**

- **数据科学家**：需要在不牺牲研究时间的前提下产出详尽的评估文档
- **ML 团队负责人**：需要跨所有模型项目保持一致、可对比的报告格式，用于组合级别评审
- **产品经理**：需要在批准上线前以业务语言理解模型表现
- **合规与风险管理人员**（金融科技、医疗科技等受监管行业）：需要有文档证明模型性能在上线前经过了严格评估

---

## 实用提示词

**提示词 1 — 基于分类指标生成完整评估报告**
```
我使用 XGBoost 训练了一个二分类模型，用于预测客户流失。以下是在留出测试集（n=[测试集大小]，正例比例 [正例比率]%）上的评估结果：

分类报告：
[粘贴 sklearn classification_report 输出]

AUC-ROC：[数值]
对数损失：[数值]
Brier 评分：[数值]

混淆矩阵：
[粘贴混淆矩阵]

业务背景：假负例（将流失用户预测为留存）每次损失 ¥[假负例成本]；假正例（将留存用户预测为流失）每次产生 ¥[假正例成本] 的不必要留存支出。

请生成完整的模型评估报告，包括：(1) 浅显易懂的执行摘要，(2) 逐指标分析与业务解读，(3) 基于业务成本的最优决策阈值建议，(4) 风险标记和局限性，(5) 部署前的后续建议步骤。
```

**提示词 2 — 多模型对比报告**
```
我针对 [预测目标] 任务运行了三组模型实验，需要为模型评审会议生成对比评估报告。

模型 A（[模型A名称]，[超参数]）：
- 验证指标：[指标]
- 训练时间：[时间]，推理延迟 p95：[延迟]

模型 B（[模型B名称]，[超参数]）：
- 验证指标：[指标]
- 训练时间：[时间]，推理延迟 p95：[延迟]

模型 C（[模型C名称]，[超参数]）：
- 验证指标：[指标]
- 训练时间：[时间]，推理延迟 p95：[延迟]

生产约束：最大推理延迟 [最大延迟] ms，最大内存 [最大内存] MB。

请撰写结构化对比报告，推荐其中一个模型，在与其他模型的对比中为推荐理由提供支撑，并明确讨论延迟与准确率之间的权衡。
```

**提示词 3 — 回归模型评估报告**
```
我构建了一个回归模型（[模型类型]），用于预测 [目标变量]（应用场景：[使用场景]）。测试集评估结果：

RMSE：[数值]
MAE：[数值]
MAPE：[数值]%
R²：[数值]
最大误差：[数值]
残差：[描述残差模式或粘贴残差统计]

该模型将用于 [下游用途，如"定价"/"库存预测"]。预测误差超过 [阈值] 将导致 [业务后果]。

请生成评估报告，内容包括：用浅显语言解释每项指标、解读残差模式、识别模型失效区间（高误差片段），并给出附带条件的生产部署建议（是否上线）。
```

**提示词 4 — 面向高管受众的评估报告**
```
我需要向 [受众，如"产品副总裁和CFO"] 汇报 [模型名称] 的评估结果，他们 ML 背景有限。该模型的功能是 [模型功能描述]。

核心指标：
[粘贴指标]

上一版本（基线）指标：
[粘贴基线指标]

请将上述评估结果改写为 1 页高管摘要：以业务影响开篇而非技术指标、将精确率/召回率转化为业务结果表述、清晰呈现与上一版本相比的改进之处，并以明确的部署建议结尾。避免使用术语——如必须使用技术词汇，请用一句话加以定义。
```

**提示词 5 — 面向合规审计的模型评估报告**
```
我们正在为 [监管框架，如"SOC 2"/"欧盟AI法案"/"SR 11-7"] 合规审查准备模型评估档案。该模型 [功能描述]，用于 [受监管场景]。

技术评估结果：
[粘贴所有指标]

训练数据：[数据描述，规模、时间范围、来源]
测试数据：[数据描述]
已知局限性：[列出局限性]

请生成符合合规要求的模型评估章节，内容涵盖：模型目的与适用范围、评估方法论及测试集独立性说明、带置信区间的性能指标、已识别的局限性及缓解措施、适合审计审阅的证明语言。
```
